<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  






























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Machine learning modelling for multi-order human visual motion processing | Perceptual Representation and Information ModEling Lab</title>

<link rel="icon" href="/prime-lab-tw-website/images/icon.png">

<meta name="title" content="Machine learning modelling for multi-order human visual motion processing">
<meta name="description" content="Delve into the mind, depict with the beauty of math.">

<meta property="og:title" content="Machine learning modelling for multi-order human visual motion processing">
<meta property="og:site_title" content="Perceptual Representation and Information ModEling Lab">
<meta property="og:description" content="Delve into the mind, depict with the beauty of math.">
<meta property="og:url" content="https://prime-psy-lab.github.io/prime-lab-tw-website">
<meta property="og:image" content="/prime-lab-tw-website/images/share.jpg">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Machine learning modelling for multi-order human visual motion processing">
<meta property="twitter:description" content="Delve into the mind, depict with the beauty of math.">
<meta property="twitter:url" content="https://prime-psy-lab.github.io/prime-lab-tw-website">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="/prime-lab-tw-website/images/share.jpg">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Machine learning modelling for multi-order human visual motion processing",
    "description": "Delve into the mind, depict with the beauty of math.",
    "headline": "Machine learning modelling for multi-order human visual motion processing",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/prime-lab-tw-website/images/icon.png" }
    },
    "url": "https://prime-psy-lab.github.io/prime-lab-tw-website"
  }
</script>

<link rel="alternate" type="application/rss+xml" href="https://prime-psy-lab.github.io/prime-lab-tw-website/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.7.0/css/all.css" rel="stylesheet" media="none" onload="this.removeAttribute('media'); this.onload = null;">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.7.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/prime-lab-tw-website/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/all.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/background.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/body.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/button.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/card.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/code.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/details.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/float.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/font.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/form.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/header.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/image.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/link.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/list.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/main.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/section.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/table.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/prime-lab-tw-website/_styles/util.css" rel="stylesheet">
  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>
<script src="/prime-lab-tw-website/_scripts/activity-album.js"></script>

<!-- include all js in scripts folder -->


  <script src="/prime-lab-tw-website/_scripts/activity-album"></script>

  <script src="/prime-lab-tw-website/_scripts/anchors.js"></script>

  <script src="/prime-lab-tw-website/_scripts/dark-mode.js"></script>

  <script src="/prime-lab-tw-website/_scripts/fetch-tags.js"></script>

  <script src="/prime-lab-tw-website/_scripts/search.js"></script>

  <script src="/prime-lab-tw-website/_scripts/site-search.js"></script>

  <script src="/prime-lab-tw-website/_scripts/table-wrap.js"></script>

  <script src="/prime-lab-tw-website/_scripts/tooltip.js"></script>


<script>
// ===========================
// Publications – highlight carousel
// ===========================
document.addEventListener("DOMContentLoaded", function () {
  const carousel = document.querySelector(".pub-carousel");
  if (!carousel) return;

  const track = carousel.querySelector(".pub-carousel-track");
  const slides = Array.from(track.children);
  const prevBtn = carousel.querySelector(".pub-carousel-prev");
  const nextBtn = carousel.querySelector(".pub-carousel-next");

  if (!slides.length) return;

  let index = 0;

  function updateCarousel() {
    // 每張卡片寬度是 100%，直接位移整個 track
    track.style.transform = "translateX(" + (-index * 100) + "%)";
  }

  prevBtn.addEventListener("click", function () {
    index = (index - 1 + slides.length) % slides.length;
    updateCarousel();
  });

  nextBtn.addEventListener("click", function () {
    index = (index + 1) % slides.length;
    updateCarousel();
  });

  updateCarousel();
});
</script>

</head>

  <body>
    







<header class="background" style="--image: url('/prime-lab-tw-website/images/background.jpg')" data-dark="true">
  <a href="/prime-lab-tw-website/" class="home">
    
      <span class="logo">
        <img src="/prime-lab-tw-website/images/icon.png" alt="logo">
      </span>
    
    
      <span class="title-text" data-tooltip="Home">
        
          <span class="title">Perceptual Representation and Information ModEling Lab</span>
        
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/prime-lab-tw-website/research/" data-tooltip="Research direction, projects, environment">
          Research
        </a>
      
    
      
        <a href="/prime-lab-tw-website/publication/" data-tooltip="Publications">
          Publications
        </a>
      
    
      
        <a href="/prime-lab-tw-website/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/prime-lab-tw-website/blog/" data-tooltip="News and inspiration">
          Blog
        </a>
      
    
      
        <a href="/prime-lab-tw-website/activity/" data-tooltip="lab research, activity gallery">
          Activity
        </a>
      
    
      
        <a href="/prime-lab-tw-website/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - filter out blank sections
-->






  
  
  

  <section class="background" data-size="1" style="--image: url('/prime-lab-tw-website/images/Research_Intro_image/motion-model/Mario_model.gif')">
    <!--
  <background>/images/Research_Intro_image/motion-model/Mario_model.gif</background>
  <dark></dark>
  <size>1</size>
-->


<h1 class="center">Machine learning modelling for multi-order human visual motion processing</h1>
  <p class="center">
    


  </p>
<div class="tags">
    
      <a href="/prime-lab-tw-website/research?search=%22tag:%20motion-perception%22" class="tag" data-tooltip='Show items with the tag "motion-perception"'>
        motion-perception
      </a>
    
      <a href="/prime-lab-tw-website/research?search=%22tag:%20deep-learning%22" class="tag" data-tooltip='Show items with the tag "deep-learning"'>
        deep-learning
      </a>
    
      <a href="/prime-lab-tw-website/research?search=%22tag:%20computational-modeling%22" class="tag" data-tooltip='Show items with the tag "computational-modeling"'>
        computational-modeling
      </a>
    
      <a href="/prime-lab-tw-website/research?search=%22tag:%20first-order-motion%22" class="tag" data-tooltip='Show items with the tag "first-order-motion"'>
        first-order-motion
      </a>
    
      <a href="/prime-lab-tw-website/research?search=%22tag:%20second-order-motion%22" class="tag" data-tooltip='Show items with the tag "second-order-motion"'>
        second-order-motion
      </a>
    
  </div>


  
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  <background></background>
  <dark></dark>
  <size></size>
-->


<h1 id="machine-learning-modelling-for-multi-order-human-visual-motion-processing">Machine learning modelling for multi-order human visual motion processing</h1>

<p>This project develops a biologically inspired motion perception model that
can process both <strong>first-order</strong> (luminance-based) and <strong>higher-order</strong>
feature motion, and compares its behavior with human vision in natural
scenes.</p>

<p>The core idea is to combine <strong>trainable motion-energy sensing</strong> with a
<strong>graph-based recurrent integration network</strong>, and to ask under what
training environments the model naturally acquires human-like sensitivity
to second-order motion—especially for objects made of non-Lambertian
materials such as glossy, transparent, or metallic surfaces.</p>

<hr>

<h2 id="overview">Overview</h2>

<p>Classical optical-flow models in computer vision can reach or even exceed
human accuracy on standard benchmarks, but they typically rely on the
<strong>intensity conservation law</strong>, and therefore fail to capture motions that
are carried by higher-order features (e.g., contrast envelopes, dynamic
textures).</p>

<p>Psychophysical and neurophysiological studies, however, show that the
biological visual system has <strong>separate, yet interacting pathways</strong> for
first- and second-order motion.</p>

<p>In this project we:</p>

<ul>
  <li>Build a two-stage, dual-pathway motion model that mimics the <strong>V1 → MT</strong>
motion-processing hierarchy.</li>
  <li>Train the model on <strong>naturalistic videos</strong> to estimate object motion.</li>
  <li>Manipulate the <strong>material properties</strong> of moving objects to test the
hypothesis that second-order motion mechanisms are useful for
<strong>robust motion estimation under optical turbulence</strong>.</li>
  <li>Compare model responses with <strong>human psychophysics</strong> and with
conventional computer-vision models.</li>
</ul>

<hr>

<h2 id="model-and-methods">Model and methods</h2>

<h3 id="stage-i--trainable-motion-energy-sensing">Stage I – Trainable motion energy sensing</h3>

<ul>
  <li>256 motion-energy units with <strong>Gabor spatial filters</strong> and temporal
filters are implemented in a deep-learning framework.</li>
  <li>The parameters (preferred speed, direction, spatial frequency, etc.)
are <strong>trainable</strong>, so the units learn an efficient motion code from
natural videos.</li>
  <li>This stage mimics <strong>V1-like local motion detectors</strong> and can reproduce
classical findings such as component- vs. pattern-selective cells and
responses to motion illusions (reverse phi, missing fundamental, etc.).</li>
</ul>

<h3 id="stage-ii--graph-based-recurrent-integration">Stage II – Graph-based recurrent integration</h3>

<ul>
  <li>Local motion energies are treated as nodes in a <strong>fully connected
motion graph</strong>.</li>
  <li>A <strong>self-attention mechanism</strong> defines adaptive connectivity between
nodes; recurrent updates integrate local motions into a coherent
global motion field.</li>
  <li>This stage mimics <strong>MT-like integration</strong>, solves aperture problems, and
also supports <strong>motion-based instance segmentation</strong> via graph
partitioning without extra training.</li>
</ul>

<h3 id="dual-pathway-design-for-higher-order-motion">Dual-pathway design for higher-order motion</h3>

<ul>
  <li>A second sensing pathway is added <strong>before</strong> the motion-energy stage:
a stack of <strong>3D CNN layers</strong> performs nonlinear preprocessing to extract
higher-order spatiotemporal features (dynamic textures, flicker, etc.).</li>
  <li>This structure is inspired by the <strong>filter–rectify–filter</strong> framework
for second-order motion processing.</li>
  <li>Both pathways are trained end-to-end to estimate object motion from
videos.</li>
</ul>

<h3 id="training-data-and-material-manipulation">Training data and material manipulation</h3>

<ul>
  <li>Naturalistic motion datasets are used as a baseline.</li>
  <li>To probe the role of second-order mechanisms, the authors create two
versions of a motion dataset:
    <ul>
      <li>
<strong>Lambertian</strong> objects with matte surfaces.</li>
      <li>
<strong>Non-Lambertian</strong> objects with glossy, transparent, or metallic
materials that induce strong optical turbulence.</li>
    </ul>
  </li>
  <li>By training the dual-pathway model on these datasets separately, the
study tests how <strong>material statistics</strong> drive the emergence of
second-order motion sensitivity.</li>
</ul>

<hr>

<h2 id="key-findings">Key findings</h2>

<h3 id="human-aligned-motion-coding">Human-aligned motion coding</h3>

<ul>
  <li>Stage I and Stage II units naturally separate into <strong>component</strong> and
<strong>pattern</strong> motion-selective populations, paralleling V1 vs. MT
recordings.</li>
  <li>The model reproduces a range of <strong>psychophysical phenomena</strong>, including
global motion integration and context-dependent pooling, with parameter
regimes that closely match human data.</li>
</ul>

<h3 id="robust-motion-estimation-and-segmentation">Robust motion estimation and segmentation</h3>

<ul>
  <li>On benchmarks such as <strong>MPI-Sintel</strong> and <strong>KITTI</strong>, the model achieves
competitive optical-flow estimation while showing <strong>stronger alignment
with human perceptual errors</strong> than standard CV models.</li>
  <li>The graph representation in Stage II enables <strong>training-free
motion-based segmentation</strong>: simple graph bipartitioning on the motion
graph yields instance-like segmentation of moving objects.</li>
</ul>

<h3 id="acquisition-of-second-order-motion-perception">Acquisition of second-order motion perception</h3>

<ul>
  <li>When trained only on standard motion datasets, the model fails to
perceive second-order motion—mirroring current CV models.</li>
  <li>When trained on datasets containing <strong>non-Lambertian materials</strong> with
optical turbulence, the dual-pathway model:
    <ul>
      <li>Spontaneously develops <strong>second-order motion sensitivity</strong> comparable
to human observers.</li>
      <li>Uses higher-order signals to <strong>stabilize object motion estimation</strong>
in the presence of noisy first-order flow.</li>
    </ul>
  </li>
  <li>These results support the hypothesis that second-order motion
mechanisms in biological vision may have evolved to <strong>robustly track
object motion under complex material-dependent optics</strong>.</li>
</ul>

<hr>

<h2 id="figure">Figure</h2>

<p><img src="/prime-lab-tw-website/images/Research_Intro_image/motion-model/image.png" alt="Dual-pathway motion perception model">
<img src="/prime-lab-tw-website/images/Research_Intro_image/motion-model/modeldemo.gif" alt="Some fantastic model output"></p>

<p><em>Figure. Overview of the two-stage dual-pathway motion model. Stage I
implements trainable motion-energy sensors for first- and higher-order
motions; Stage II integrates local motion via a graph-based recurrent
network to infer object motion and segment moving objects.</em></p>

<h2 id="related-publications">Related publications</h2>

<p>Sun, Z., <strong>Chen, Y.-J.</strong>, Yang, Y.-H., Li, Y., &amp; Nishida, S.
Machine learning modelling for multi-order human visual motion
processing. Nature Machine Intelligence, 2025.
DOI: 10.1038/s42256-025-01068-w</p>

<p>Sun, Z., <strong>Chen, Y.-J.</strong>, Yang, Y.-H., &amp; Nishida, S.
Modelling Human Visual Motion Processing with Trainable Motion Energy
Sensing and a Self-attention Network. arXiv:2305.09156, 2023.
DOI: 10.48550/arXiv.2305.09156</p>

<p>Sun, Z., <strong>Chen, Y.-J.</strong>, Yang, Y.-H., &amp; Nishida, S.
Modeling of Human Motion Perception Mechanism: A Simulation based on
Deep Neural Network and Attention Transformer. Journal of Vision,
23(9), 4894, 2023.
DOI: 10.1167/jov.23.9.4894</p>

<p>Sun, Z., <strong>Chen, Y.-J.</strong>, Yang, Y.-H., &amp; Nishida, S.
Acquisition of second-order motion perception by learning to recognize
the motion of objects made by non-diffusive materials. Journal of
Vision, 24(10), 374, 2024.
DOI: 10.1167/jov.24.10.374</p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  <background></background>
  <dark></dark>
  <size></size>
-->


<p class="center">
  <a href="/prime-lab-tw-website/research/">
    ← Back to Research
  </a>
</p>
  </section>


    </main>
    


<footer class="background" style="--image: url('/prime-lab-tw-website/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="mailto:prime.lab.tw@gmail.com" data-tooltip="Email" data-style="bare" aria-label="Email">
      <i class="icon fa-solid fa-envelope"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=9RKoQacAAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/PRIME-PSY-LAB" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2026
    Perceptual Representation and Information ModEling Lab
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
